{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb8e4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc1ec8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into DataFrame \n",
    "data = pd.read_csv('df_KO_.csv',parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3e8d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Reported EPS</th>\n",
       "      <th>Price Change</th>\n",
       "      <th>RSI</th>\n",
       "      <th>K</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>EMA26</th>\n",
       "      <th>MACD Line</th>\n",
       "      <th>Signal Line</th>\n",
       "      <th>MACD Histogram</th>\n",
       "      <th>ROC</th>\n",
       "      <th>OBV</th>\n",
       "      <th>P/E</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-05-06</th>\n",
       "      <td>19.937500</td>\n",
       "      <td>20.218750</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>20.218750</td>\n",
       "      <td>10.236874</td>\n",
       "      <td>7170400</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>20.167723</td>\n",
       "      <td>20.122550</td>\n",
       "      <td>0.045173</td>\n",
       "      <td>0.045976</td>\n",
       "      <td>-0.000803</td>\n",
       "      <td>-0.154321</td>\n",
       "      <td>-10631600</td>\n",
       "      <td>144.419643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-05-07</th>\n",
       "      <td>20.218750</td>\n",
       "      <td>20.406250</td>\n",
       "      <td>20.156250</td>\n",
       "      <td>20.343750</td>\n",
       "      <td>10.300159</td>\n",
       "      <td>6702800</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>56.578947</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>20.194804</td>\n",
       "      <td>20.138935</td>\n",
       "      <td>0.055869</td>\n",
       "      <td>0.047955</td>\n",
       "      <td>0.007914</td>\n",
       "      <td>1.244168</td>\n",
       "      <td>-3928800</td>\n",
       "      <td>145.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-05-08</th>\n",
       "      <td>20.343750</td>\n",
       "      <td>20.687500</td>\n",
       "      <td>20.062500</td>\n",
       "      <td>20.687500</td>\n",
       "      <td>10.474213</td>\n",
       "      <td>8292800</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>65.853659</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>20.270603</td>\n",
       "      <td>20.179569</td>\n",
       "      <td>0.091034</td>\n",
       "      <td>0.056571</td>\n",
       "      <td>0.034463</td>\n",
       "      <td>1.846154</td>\n",
       "      <td>4364000</td>\n",
       "      <td>147.767857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-05-09</th>\n",
       "      <td>20.687500</td>\n",
       "      <td>20.937500</td>\n",
       "      <td>20.593750</td>\n",
       "      <td>20.687500</td>\n",
       "      <td>10.474213</td>\n",
       "      <td>4820400</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.052632</td>\n",
       "      <td>78.947368</td>\n",
       "      <td>20.334741</td>\n",
       "      <td>20.217194</td>\n",
       "      <td>0.117547</td>\n",
       "      <td>0.068766</td>\n",
       "      <td>0.048781</td>\n",
       "      <td>1.846154</td>\n",
       "      <td>4364000</td>\n",
       "      <td>147.767857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-05-10</th>\n",
       "      <td>20.718750</td>\n",
       "      <td>20.968750</td>\n",
       "      <td>20.718750</td>\n",
       "      <td>20.968750</td>\n",
       "      <td>10.616608</td>\n",
       "      <td>4942800</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>68.571429</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>20.432281</td>\n",
       "      <td>20.272865</td>\n",
       "      <td>0.159416</td>\n",
       "      <td>0.086896</td>\n",
       "      <td>0.072520</td>\n",
       "      <td>3.389831</td>\n",
       "      <td>9306800</td>\n",
       "      <td>149.776786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>63.500000</td>\n",
       "      <td>63.869999</td>\n",
       "      <td>63.200001</td>\n",
       "      <td>63.820000</td>\n",
       "      <td>62.855492</td>\n",
       "      <td>6463300</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>53.333314</td>\n",
       "      <td>62.869238</td>\n",
       "      <td>63.351839</td>\n",
       "      <td>62.744877</td>\n",
       "      <td>0.606962</td>\n",
       "      <td>0.768770</td>\n",
       "      <td>-0.161808</td>\n",
       "      <td>-0.234487</td>\n",
       "      <td>3006553500</td>\n",
       "      <td>92.492753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>63.930000</td>\n",
       "      <td>64.290001</td>\n",
       "      <td>63.709999</td>\n",
       "      <td>64.209999</td>\n",
       "      <td>63.239597</td>\n",
       "      <td>7320700</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.389999</td>\n",
       "      <td>56.862744</td>\n",
       "      <td>79.324942</td>\n",
       "      <td>63.483864</td>\n",
       "      <td>62.853404</td>\n",
       "      <td>0.630459</td>\n",
       "      <td>0.741108</td>\n",
       "      <td>-0.110649</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>3013874200</td>\n",
       "      <td>93.057970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>64.459999</td>\n",
       "      <td>64.650002</td>\n",
       "      <td>63.490002</td>\n",
       "      <td>63.570000</td>\n",
       "      <td>62.609272</td>\n",
       "      <td>7159400</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.639999</td>\n",
       "      <td>50.243892</td>\n",
       "      <td>52.320693</td>\n",
       "      <td>63.497115</td>\n",
       "      <td>62.906485</td>\n",
       "      <td>0.590630</td>\n",
       "      <td>0.711012</td>\n",
       "      <td>-0.120382</td>\n",
       "      <td>-0.656356</td>\n",
       "      <td>3006714800</td>\n",
       "      <td>92.130434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>63.799999</td>\n",
       "      <td>64.150002</td>\n",
       "      <td>63.700001</td>\n",
       "      <td>63.950001</td>\n",
       "      <td>62.983532</td>\n",
       "      <td>7169300</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.380001</td>\n",
       "      <td>51.118206</td>\n",
       "      <td>68.354526</td>\n",
       "      <td>63.566790</td>\n",
       "      <td>62.983783</td>\n",
       "      <td>0.583007</td>\n",
       "      <td>0.685411</td>\n",
       "      <td>-0.102404</td>\n",
       "      <td>1.331010</td>\n",
       "      <td>3013884100</td>\n",
       "      <td>92.681161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>63.919998</td>\n",
       "      <td>63.919998</td>\n",
       "      <td>63.169998</td>\n",
       "      <td>63.610001</td>\n",
       "      <td>62.648666</td>\n",
       "      <td>7650200</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.340000</td>\n",
       "      <td>53.962911</td>\n",
       "      <td>54.008499</td>\n",
       "      <td>63.573438</td>\n",
       "      <td>63.030169</td>\n",
       "      <td>0.543268</td>\n",
       "      <td>0.656983</td>\n",
       "      <td>-0.113714</td>\n",
       "      <td>1.370519</td>\n",
       "      <td>3006233900</td>\n",
       "      <td>92.188407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6711 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close   Volume  \\\n",
       "Date                                                                         \n",
       "1996-05-06  19.937500  20.218750  19.750000  20.218750  10.236874  7170400   \n",
       "1996-05-07  20.218750  20.406250  20.156250  20.343750  10.300159  6702800   \n",
       "1996-05-08  20.343750  20.687500  20.062500  20.687500  10.474213  8292800   \n",
       "1996-05-09  20.687500  20.937500  20.593750  20.687500  10.474213  4820400   \n",
       "1996-05-10  20.718750  20.968750  20.718750  20.968750  10.616608  4942800   \n",
       "...               ...        ...        ...        ...        ...      ...   \n",
       "2022-12-23  63.500000  63.869999  63.200001  63.820000  62.855492  6463300   \n",
       "2022-12-27  63.930000  64.290001  63.709999  64.209999  63.239597  7320700   \n",
       "2022-12-28  64.459999  64.650002  63.490002  63.570000  62.609272  7159400   \n",
       "2022-12-29  63.799999  64.150002  63.700001  63.950001  62.983532  7169300   \n",
       "2022-12-30  63.919998  63.919998  63.169998  63.610001  62.648666  7650200   \n",
       "\n",
       "            Reported EPS  Price Change        RSI           K      EMA12  \\\n",
       "Date                                                                       \n",
       "1996-05-06          0.14      0.281250  54.166667   70.000000  20.167723   \n",
       "1996-05-07          0.14      0.125000  56.578947   83.333333  20.194804   \n",
       "1996-05-08          0.14      0.343750  65.853659  100.000000  20.270603   \n",
       "1996-05-09          0.14      0.000000  71.052632   78.947368  20.334741   \n",
       "1996-05-10          0.14      0.281250  68.571429  100.000000  20.432281   \n",
       "...                  ...           ...        ...         ...        ...   \n",
       "2022-12-23          0.69      0.480000  53.333314   62.869238  63.351839   \n",
       "2022-12-27          0.69      0.389999  56.862744   79.324942  63.483864   \n",
       "2022-12-28          0.69     -0.639999  50.243892   52.320693  63.497115   \n",
       "2022-12-29          0.69      0.380001  51.118206   68.354526  63.566790   \n",
       "2022-12-30          0.69     -0.340000  53.962911   54.008499  63.573438   \n",
       "\n",
       "                EMA26  MACD Line  Signal Line  MACD Histogram       ROC  \\\n",
       "Date                                                                      \n",
       "1996-05-06  20.122550   0.045173     0.045976       -0.000803 -0.154321   \n",
       "1996-05-07  20.138935   0.055869     0.047955        0.007914  1.244168   \n",
       "1996-05-08  20.179569   0.091034     0.056571        0.034463  1.846154   \n",
       "1996-05-09  20.217194   0.117547     0.068766        0.048781  1.846154   \n",
       "1996-05-10  20.272865   0.159416     0.086896        0.072520  3.389831   \n",
       "...               ...        ...          ...             ...       ...   \n",
       "2022-12-23  62.744877   0.606962     0.768770       -0.161808 -0.234487   \n",
       "2022-12-27  62.853404   0.630459     0.741108       -0.110649  0.343800   \n",
       "2022-12-28  62.906485   0.590630     0.711012       -0.120382 -0.656356   \n",
       "2022-12-29  62.983783   0.583007     0.685411       -0.102404  1.331010   \n",
       "2022-12-30  63.030169   0.543268     0.656983       -0.113714  1.370519   \n",
       "\n",
       "                   OBV         P/E  \n",
       "Date                                \n",
       "1996-05-06   -10631600  144.419643  \n",
       "1996-05-07    -3928800  145.312500  \n",
       "1996-05-08     4364000  147.767857  \n",
       "1996-05-09     4364000  147.767857  \n",
       "1996-05-10     9306800  149.776786  \n",
       "...                ...         ...  \n",
       "2022-12-23  3006553500   92.492753  \n",
       "2022-12-27  3013874200   93.057970  \n",
       "2022-12-28  3006714800   92.130434  \n",
       "2022-12-29  3013884100   92.681161  \n",
       "2022-12-30  3006233900   92.188407  \n",
       "\n",
       "[6711 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a46ee0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column 'y' to store the 'Close' values\n",
    "data['y'] = data['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecee43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all columns for x and the 'y' column for y\n",
    "x = data.iloc[:, :18].values\n",
    "y = data.iloc[:, 18].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "963bb235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX: (5368, 18) trainY: (5368,)\n",
      "testX: (1343, 18) testY: (1343,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training and testing sets (80-20 split)\n",
    "split = int(data.shape[0]* 0.8)\n",
    "train_x, test_x = x[: split, :], x[split:, :]\n",
    "train_y, test_y = y[: split, ], y[split: , ]\n",
    "\n",
    "# Check the shapes of the training and testing datasets\n",
    "print(f'trainX: {train_x.shape} trainY: {train_y.shape}')\n",
    "print(f'testX: {test_x.shape} testY: {test_y.shape}')\n",
    "\n",
    "# Initializing MinMaxScaler to scale the features and labels between 0 and 1\n",
    "x_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "y_scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "# Fitting and transforming the training features and labels\n",
    "train_x = x_scaler.fit_transform(train_x)\n",
    "test_x = x_scaler.transform(test_x)\n",
    "\n",
    "# Transforming the testing features and labels\n",
    "train_y = y_scaler.fit_transform(train_y.reshape(-1, 1))\n",
    "test_y = y_scaler.transform(test_y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f69979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definition of sliding_window function\n",
    "def sliding_window(x, y, window):\n",
    "\n",
    "\"\"\"\n",
    "`sliding_window` Function\n",
    "- The sliding_window function is used to transform time-series data into a suitable \n",
    "  format for training the GAN.\n",
    "- It takes input arrays `x` and `y`, and a `window` size as input.\n",
    "- The function slides the window across the `x` and extracts corresponding windows of \n",
    "  `x` and `y` data.\n",
    "- The function returns these windows as new `x` and `y` tensors, as well as additional\n",
    "  `y` tensors for the GAN.\n",
    "\"\"\"\n",
    "    \n",
    "    x_ = []    # to store slices of x\n",
    "    y_ = []    # to store corresponding y values\n",
    "    y_gan = [] # to store slices of y for GAN\n",
    "    \n",
    "    for i in range(window, x.shape[0]):\n",
    "        # extracting a window of data from x and y\n",
    "        tmp_x = x[i - window: i, :]\n",
    "        tmp_y = y[i]\n",
    "        tmp_y_gan = y[i - window: i + 1]\n",
    "        # appending the extracted data to the lists\n",
    "        x_.append(tmp_x)\n",
    "        y_.append(tmp_y)\n",
    "        y_gan.append(tmp_y_gan)\n",
    "    # converting lists to PyTorch tensors\n",
    "    x_ = torch.from_numpy(np.array(x_)).float()\n",
    "    y_ = torch.from_numpy(np.array(y_)).float()\n",
    "    y_gan = torch.from_numpy(np.array(y_gan)).float()\n",
    "    return x_, y_, y_gan\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3dc228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: torch.Size([5365, 3, 18]) train_y: torch.Size([5365, 1]) train_y_gan: torch.Size([5365, 4, 1])\n",
      "test_x: torch.Size([1340, 3, 18]) test_y: torch.Size([1340, 1]) test_y_gan: torch.Size([1340, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# Call to sliding_window function for both training and testing data\n",
    "train_x_slide, train_y_slide, train_y_gan = sliding_window(train_x, train_y, 3)\n",
    "test_x_slide, test_y_slide, test_y_gan = sliding_window(test_x, test_y, 3)\n",
    "\n",
    "# Checking the shapes of the tensors\n",
    "print(f'train_x: {train_x_slide.shape} train_y: {train_y_slide.shape} train_y_gan: {train_y_gan.shape}')\n",
    "print(f'test_x: {test_x_slide.shape} test_y: {test_y_slide.shape} test_y_gan: {test_y_gan.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa1fb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "`Generator` Class\n",
    "- The Generator is a neural network class with three GRU layers and three Linear layers.\n",
    "- It takes an input tensor and outputs a tensor of a single dimension.\n",
    "- The forward method of the class performs the forward pass of the network.\n",
    "- The forward pass involves passing the input through the GRU layers, applying dropout \n",
    "  to the output, and then passing the result through the Linear layers.\n",
    "- The network uses CUDA if available for GPU acceleration.\n",
    "\"\"\"\n",
    "\n",
    "# Definition of Generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.gru_1 = nn.GRU(input_size, 1024, batch_first = True)\n",
    "        self.gru_2 = nn.GRU(1024, 512, batch_first = True)\n",
    "        self.gru_3 = nn.GRU(512, 256, batch_first = True)\n",
    "        self.linear_1 = nn.Linear(256, 128)\n",
    "        self.linear_2 = nn.Linear(128, 64)\n",
    "        self.linear_3 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    # Defining the forward pass\n",
    "    def forward(self, x):\n",
    "        # Checking CUDA availability\n",
    "        use_cuda = 1\n",
    "        device = torch.device(\"cuda\" if (torch.cuda.is_available() & use_cuda) else \"cpu\")\n",
    "        h0 = torch.zeros(1, x.size(0), 1024).to(device)  # Initial hidden state\n",
    "        out_1, _ = self.gru_1(x, h0)  # Passing through first GRU layer\n",
    "        out_1 = self.dropout(out_1)\n",
    "        h1 = torch.zeros(1, x.size(0), 512).to(device)\n",
    "        out_2, _ = self.gru_2(out_1, h1)\n",
    "        out_2 = self.dropout(out_2)\n",
    "        h2 = torch.zeros(1, x.size(0), 256).to(device)\n",
    "        out_3, _ = self.gru_3(out_2, h2)\n",
    "        out_3 = self.dropout(out_3)\n",
    "        out_4 = self.linear_1(out_3[:, -1, :])\n",
    "        out_5 = self.linear_2(out_4)\n",
    "        out = self.linear_3(out_5)\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "`Discriminator` Class\n",
    "- The Discriminator is a neural network class with three 1D Convolutional layers and three Linear layers.\n",
    "- It takes a 1D tensor as input and outputs a single value between 0 and 1, representing the \n",
    "  probability that the input is real (as opposed to generated).\n",
    "- The forward method of the class performs the forward pass of the network.\n",
    "- The forward pass involves passing the input through the Convolutional layers, flattening \n",
    "  the output, and then passing the result through the Linear layers.\n",
    "- The network uses LeakyReLU activation functions for the Convolutional layers and ReLU \n",
    "  for the Linear layers, and it applies Sigmoid activation to the output.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Definition of Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Defining Conv and Linear layers\n",
    "        self.conv1 = nn.Conv1d(4, 32, kernel_size = 3, stride = 1, padding = 'same')\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size = 3, stride = 1, padding = 'same')\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size = 3, stride = 1, padding = 'same')\n",
    "        self.linear1 = nn.Linear(128, 220)\n",
    "        self.batch1 = nn.BatchNorm1d(220)\n",
    "        self.linear2 = nn.Linear(220, 220)\n",
    "        self.batch2 = nn.BatchNorm1d(220)\n",
    "        self.linear3 = nn.Linear(220, 1)\n",
    "        # Activation layers\n",
    "        self.leaky = nn.LeakyReLU(0.01)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # Defining the forward pass\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)      \n",
    "        conv1 = self.leaky(conv1)   \n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv2 = self.leaky(conv2)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv3 = self.leaky(conv3)\n",
    "        flatten_x = conv3.reshape(conv3.shape[0], conv3.shape[1]) \n",
    "        out_1 = self.linear1(flatten_x)\n",
    "        out_1 = self.leaky(out_1)\n",
    "        out_2 = self.linear2(out_1)\n",
    "        out_2 = self.relu(out_2)\n",
    "        out_3 = self.linear3(out_2)\n",
    "        out = self.sigmoid(out_3)    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8237286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/165] LossD: 58.228445291519165 LossG:29.326889872550964\n"
     ]
    }
   ],
   "source": [
    "# Setting up CUDA for GPU acceleration, if available\n",
    "use_cuda = 1\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() & use_cuda) else \"cpu\")\n",
    "\n",
    "# Hyperparameters for training\n",
    "batch_size = 128\n",
    "learning_rate = 0.00016\n",
    "num_epochs = 165\n",
    "\n",
    "# Creating DataLoader for batching training data\n",
    "trainDataloader = DataLoader(TensorDataset(train_x_slide, train_y_gan), batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# Initializing the Generator and Discriminator models\n",
    "modelG = Generator(18).to(device) # Updated input size\n",
    "modelD = Discriminator().to(device)\n",
    "\n",
    "# Setting the loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizerG = torch.optim.Adam(modelG.parameters(), lr = learning_rate, betas = (0.0, 0.9))\n",
    "optimizerD = torch.optim.Adam(modelD.parameters(), lr = learning_rate, betas = (0.0, 0.9))\n",
    "\n",
    "# Arrays to keep track of losses over epochs\n",
    "histG = np.zeros(num_epochs)\n",
    "histD = np.zeros(num_epochs)\n",
    "count = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    loss_G = []\n",
    "    loss_D = []\n",
    "    for (x, y) in trainDataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Generate fake data with the Generator\n",
    "        fake_data = modelG(x)\n",
    "        fake_data = torch.cat([y[:, :3, :], fake_data.reshape(-1, 1, 1)], axis = 1)\n",
    "\n",
    "        # Discriminator's loss calculation on real data\n",
    "        dis_real_output = modelD(y)\n",
    "        real_labels = torch.ones_like(dis_real_output).to(device)\n",
    "        lossD_real = criterion(dis_real_output, real_labels)\n",
    "\n",
    "        # Discriminator's loss calculation on fake data\n",
    "        dis_fake_output = modelD(fake_data)\n",
    "        fake_labels = torch.zeros_like(real_labels).to(device)\n",
    "        lossD_fake = criterion(dis_fake_output, fake_labels)\n",
    "        \n",
    "        # Total Discriminator's loss\n",
    "        lossD = (lossD_real + lossD_fake)\n",
    "        \n",
    "        # Zero gradients, backpropagation, and optimization for Discriminator\n",
    "        modelD.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        optimizerD.step()\n",
    "        loss_D.append(lossD.item())\n",
    "        \n",
    "        # Generator's loss calculation\n",
    "        output_fake = modelD(fake_data)\n",
    "        lossG = criterion(output_fake, real_labels)\n",
    "        \n",
    "        # Zero gradients, backpropagation, and optimization for Generator\n",
    "        modelG.zero_grad()\n",
    "        lossG.backward()\n",
    "        optimizerG.step()\n",
    "        loss_G.append(lossG.item()) \n",
    "\n",
    "    # Storing the average losses for this epoch\n",
    "    histG[epoch] = sum(loss_G) \n",
    "    histD[epoch] = sum(loss_D)    \n",
    "    \n",
    "    # Print losses for this epoch\n",
    "    print(f'[{epoch+1}/{num_epochs}] LossD: {sum(loss_D)} LossG:{sum(loss_G)}')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Generator and Discriminator Loss\n",
    "plt.figure(figsize = (12, 6))\n",
    "plt.plot(histG, color = 'blue', label = 'Generator Loss')\n",
    "plt.plot(histD, color = 'black', label = 'Discriminator Loss')\n",
    "plt.title('GAN Loss')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(loc = 'upper right')\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f7bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Generator to evaluation mode\n",
    "modelG.eval()\n",
    "# Getting the predictions for training and testing data\n",
    "pred_y_train = modelG(train_x_slide.to(device))\n",
    "pred_y_test = modelG(test_x_slide.to(device))\n",
    "\n",
    "# Inverting the scaling for true and predicted values for comparison\n",
    "y_train_true = y_scaler.inverse_transform(train_y_slide)\n",
    "y_train_pred = y_scaler.inverse_transform(pred_y_train.cpu().detach().numpy())\n",
    "\n",
    "y_test_true = y_scaler.inverse_transform(test_y_slide)\n",
    "y_test_pred = y_scaler.inverse_transform(pred_y_test.cpu().detach().numpy())\n",
    "     \n",
    "# Plotting the actual vs predicted prices for the training dataset\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(y_train_true, color = 'red', label = 'Acutal Price')\n",
    "plt.plot(y_train_pred, color = 'blue', label = 'Predict Price')\n",
    "plt.title('GAN prediction training dataset')\n",
    "plt.ylabel('TWD')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "# Calculating and printing the Root Mean Squared Error for the training dataset\n",
    "MSE = mean_squared_error(y_train_true, y_train_pred)\n",
    "RMSE = math.sqrt(MSE)\n",
    "print(f'Training dataset RMSE:{RMSE}')\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the actual vs predicted prices for the testing dataset\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(y_test_true, color = 'red', label = 'Acutal Price')\n",
    "plt.plot(y_test_pred, color = 'blue', label = 'Predict Price')\n",
    "plt.title('GAN prediction testing dataset')\n",
    "plt.ylabel('TWD')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "# Calculating and printing the Root Mean Squared Error for the testing dataset\n",
    "MSE = mean_squared_error(y_test_true, y_test_pred)\n",
    "RMSE = math.sqrt(MSE)\n",
    "print(f'Test dataset RMSE:{RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36af820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
